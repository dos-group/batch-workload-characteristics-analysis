# Article, Year, Layers, Reference, Keywords
"Performance Prediction for Apache Spark Platform","2015","Performance Layer, Hardware Layer","wang_performance_2015","(de)serialization time, Start and Cleanup Time, Number of Nodes, Number of CPU Cores per Node, Task I/O Read Cost, Task I/O Write Cost, Task Memory Requirement"
"Hadoop Performance Self-Tuning Using a Fuzzy-Prediction Approach","2016","Hardware Layer, Big Data Framework Layer","lee_hadoop_2016","Number of Slots, Number of CPU Cores, Memory Size"
"Comparison and Improvement of Hadoop MapReduce Performance Prediction Models in the Private Cloud","2016","Big Data Framework Layer, User Application Layer","shen_novel_2023","Framework Parameter Configuration, Job parameters, Application Type, Data Input Size, Number of Nodes"
"BigExplorer: A configuration recommendation system for big data platform","2016","Big Data Framework Layer, User Application Layer, Performance Layer","yeh_bigexplorer_2016","Framework Parameter Configuration, Job History, System Logs, Workload Type, Historical Execution Time"
"Performance modeling for spark using SVM","2016","Big Data Framework Layer","luo_performance_2016","Framework Parameter Configuration"
"Selecting resources for distributed dataflow systems according to runtime targets","2016","Performance Layer, Data Layer, User Application Layer, Big Data Framework Layer, Hardware Layer","thamsen_selecting_2016","Input Parameters, Data Input Size, Basic information on previously executed jobs, Number of CPU Cores, Memory Size"
"Continuously improving the resource utilization of iterative parallel dataflows","2016","Performance Layer, Big Data Framework Layer","thamsen_continuously_2016","CPU Utilisation - CPU Time, Available Resources, Sensable Lower Bound Ressource Utilisation, JVM CPU Time, Workflow Structure (Spark DAG)"
"Support vector regression model for BigData systems","2016","Performance Layer, Data Layer, Big Data Framework Layer","rizzi_support_2016","Workflow Structure (Spark DAG), Number of CPU Cores, Historical Execution Time, Data Input Size, number of bytes transferred during shuﬄes"
"Estimation Accuracy on Execution Time of Run-Time Tasks in a Heterogeneous Distributed Environment","2016","Performance Layer","liu_estimation_2016","Historical Execution Time, Running Tasks, Progress of running of Tasks, Task Timestamps"
"Ellis: dynamically scaling distributed dataflows to meet runtime targets","2017","Performance Layer, Big Data Framework Layer","thamsen_ellis_2017","Number of Runs (Historical), Ressource Utilisation Data, Available Resources, Stage Execution Time"
"dSpark: Deadline-Based Resource Allocation for Big Data Applications in Apache Spark","2017","Big Data Framework Layer, Performance Layer, User Application Layer","islam_dspark_2017","Framework Parameter Configuration, Workload Type, Ressource Utilisation Data"
"BBQ: Elastic MapReduce over Cloud Platforms","2017","Big Data Framework Layer, Hardware Layer, Data Layer","chalvantzis_bbq_2017","Data Input Size, Number of Tasks, Number of Nodes, Number of Job Stages"
"Performance Tuning and Modeling for Big Data Applications in Docker Containers","2017","Hardware Layer, Virtualisation Layer","ye_performance_2017","CPU Utilisation - Overall, Disk Read Speed, Disk Write Speed, Memory Size per Node, Docker Configuration, resource interference"
"iSpot: achieving predictable performance for big data analytics with cloud transient servers","2017","Virtualisation Layer, Performance Layer, Big Data Framework Layer","xu_ispot_2017","Data (de)serialization time, Garbage collection time, Data Input Size, Workflow Structure (Spark DAG), Historical Execution Time, Disk bandwidth, Number of Nodes, Data processing ratio, data localization ratio, Data Processing Rate, Parallelism factor, Available Network Bandwidth"
"A collaborative filtering based approach to performance prediction for parallel applications","2017","Data Layer, Hardware Layer, Big Data Framework Layer","shao_collaborative_2017","Memory Size per Node, Slot Memory, CPU Cores per Slot, Number of Slots, number of partitions, Completed Stages, Data Input Size, Number of CPU Cores"
"Model-Based Performance Evaluation of Batch and Stream Applications for Big Data","2017","Big Data Framework Layer, User Application Layer, Hardware Layer","krob_model-based_2017","Workflow Structure (Spark DAG), Data Chunks Size, Input Parameters, Number of Nodes"
"Fluid Petri Nets for the Performance Evaluation of MapReduce and Spark Applications","2017","Big Data Framework Layer","gianniti_fluid_2017","Workflow Structure (Spark DAG), Number of Tasks, Number of Slots"
"A Combined Analytical Modeling Machine Learning Approach for Performance Prediction of MapReduce Jobs in Cloud Environment","2017","Data Layer, Performance Layer, Hardware Layer","ataie_combined_2016","Historical Execution Time, Number of CPU Cores, Data Input Size, Running Tasks, Completed Tasks, Workflow Structure (Spark DAG)"
"CherryPick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics","2017","Hardware Layer, Virtualisation Layer","alipourfard_cherrypick_nodate","Number of Cloud Instances, Number of CPU Cores, CPU Processing Power, Number of Disks, Disk Read Speed, Disk Write Speed, Network bandwidth, RAM per Core, Price per Hour"
"On MapReduce Scheduling in Hadoop Yarn on Heterogeneous Clusters","2018","Data Layer, Big Data Framework Layer, Performance Layer","wang_mapreduce_2018","Job queue, Task Memory Requirement, Task CPU Requirement, Node Manager CPU Idle, Node Manager Memory Idle, Node Manager CPU Time , Node Manager Memory, Local Map Tasks Count of a Job, Job Priority User Specified, Job Data Size, Job Dynamic Priority"
"Sparks operation time predictive in cloud computing environment based on SRC-WSVR","2018","Data Layer, Hardware Layer, Performance Layer","li_sparks_2018","(de)serialization time, Task Execution Time, Start and Cleanup Time, Number of CPU Cores per Node, Number of Nodes, Data Input Size, Memory Size per Node"
"A gray-box performance model for Apache Spark","2018","Big Data Framework Layer, Data Layer","chao_gray-box_2018","Framework Parameter Configuration, Data Input Size"
"Datasize-aware high dimensional configurations auto-tuning of in-memory cluster computing","2018","Data Layer, Big Data Framework Layer","yu_datasize-aware_2018","Data Input Size, Framework Parameter Configuration"
"Auto-tuning spark configurations based on neural network","2018","Performance Layer, Big Data Framework Layer","gu_auto-tuning_2018","Historical Execution Time, Framework Parameter Configuration"
"CoBell: Runtime Prediction for Distributed Dataflow Jobs in Shared Clusters","2018","Performance Layer, Data Layer, User Application Layer, Big Data Framework Layer, Hardware Layer","verbitskiy_cobell_2018","Input Parameters, Data Input Size, Basic information on previously executed jobs, Number of CPU Cores, Memory Size, Historical Execution Time"
"Performance Prediction of Cloud-Based Big Data Applications","2018","Big Data Framework Layer","ardagna_performance_2018","Workflow Structure (Spark DAG), Running Tasks, Completed Stages, Completed Tasks"
"Towards optimal resource provisioning for Hadoop-MapReduce jobs using scale-out strategy and its performance analysis in private cloud environment","2019","Hardware Layer, Data Layer, Big Data Framework Layer","ramanathan_towards_2019","Data Input Size, Number of Tasks, hardware speciﬁcations / machine name, Data Chunks Size, Framework Parameter Configuration"
"Cost-Effective Cloud Server Provisioning for Predictable Performance of Big Data Analytics","2019","Performance Layer, Big Data Framework Layer, Data Layer, Virtualisation Layer","xu_cost-effective_2019","Data Input Size, Number of Job Stages, (de)serialization time, Garbage collection time, Number of Tasks, Price per Hour, Cloud Machine Type, Number of Cloud Instances"
"Analytical composite performance models for Big Data applications","2019","Big Data Framework Layer","karimian-aliabadi_analytical_2019","Workflow Structure (Spark DAG)"
"PerTract: Model Extraction and Specification of Big Data Systems for Performance Prediction by the Example of Apache Spark and Hadoop","2019","Performance Layer","kros_pertract_2019","JVM CPU Time, CPU Utilisation - CPU Time, CPU Utilisation - Providing Infrastructure , Task I/O Read Cost, Network latency, CPU Utilisation - (de)serialization time, Available Network Bandwidth"
"Mapreduce performance model for Hadoop 2.x","2019","Hardware Layer, Big Data Framework Layer, Performance Layer","glushkova_mapreduce_2019","Number of Disk per Node, Number of Nodes, Framework Parameter Configuration, Number of Job Stages, Number of CPU Cores per Node, Task Execution Time, Processing Latency"
"Fast. Efficient Performance Predictions for Big Data Applications","2019","Performance Layer, Hardware Layer, Data Layer","maroulis_fast_2019","Historical Execution Time, Data Input Size, Number of CPU Cores, Memory Size"
"Scalable Performance Modeling and Evaluation of MapReduce Applications","2019","Big Data Framework Layer","ataie_combined_2016","Workflow Structure (Spark DAG), Running Tasks, Completed Tasks, Completed Stages"
"Fast and Lightweight Execution Time Predictions for Spark Applications","2019","Performance Layer, Big Data Framework Layer","amannejad_fast_2019","CPU Cores per Slot, number of partitions, Historical Execution Time"
"Quick Execution Time Predictions for Spark Applications","2019","Performance Layer, Big Data Framework Layer","amannejad_fast_2019","CPU Cores per Slot, number of partitions, Historical Execution Time, Data Input Size"
"Machine learning for performance prediction of spark cloud applications","2019","Hardware Layer, Data Layer, Performance Layer","maros_machine_2019","Data processing ratio, Number of CPU Cores, Data Input Size, Number of Tasks, Historical Execution Time"
"Estimating runtime of a job in Hadoop MapReduce","2020","Hardware Layer, Big Data Framework Layer, Data Layer","peyravi_estimating_2020","Data Input Size, Framework Parameter Configuration, Number of Nodes, Number of Slots, CPU Processing Power, Number of CPU Cores per Node, RAM Write Speed, RAM Read Speed, Disk Write Speed, Disk Read Speed, Available Network Bandwidth"
"Performance Prediction for Convolutional Neural Network on Spark Cluster","2020","Data Layer, Hardware Layer","myung_performance_2020","Data Input Size, Number of CPU Cores per Node, Number of Nodes, Memory Size per Node"
"A Performance Prediction Model for Spark Applications","2020","Big Data Framework Layer, Data Layer","maros_machine_2019","Framework Parameter Configuration, Number of Tasks, Task success rate, Completed Stages"
"Designing a MapReduce performance model in distributed heterogeneous platforms based on benchmarking approach","2020","Performance Layer, Data Layer","gandomi_designing_2020","Task Execution Time, Data Input Size, Data Output Size"
"Balance Resource Allocation for Spark Jobs Based on Prediction of the Optimal Resource","2020","Big Data Framework Layer, Hardware Layer, Data Layer","hu_balance_2020","Data Input Size, Framework Parameter Configuration, Workflow Structure (Spark DAG), Number of CPU Cores, Memory Size"
"Cross-Domain Workloads Performance Prediction via Runtime Metrics Transferring","2020","Performance Layer, Virtualisation Layer","li_cross-domain_2020","System Level Metrics, Historical Execution Time, VM Config"
"SPM: Modeling Spark Task Execution Time from the Sub-stage Perspective","2020","Data Layer, Performance Layer, Virtualisation Layer, Big Data Framework Layer","ataie_combined_2016","Data Input Size, Task Scheduler Delay, (de)serialization time, Historical Execution Time, Garbage collection time, Shuffle R/W"
"On Machine Learning-based Stage-aware Performance Prediction of Spark Applications","2020","Big Data Framework Layer, Data Layer","ye_machine_2020","Number of CPU Cores per Node, Memory Size per Node, Number of Nodes, Data Input Size, Shuffle R/W"
"Performance prediction for data-driven workflows on apache spark","2020","Hardware Layer, Data Layer, User Application Layer, Big Data Framework Layer","gulino_performance_2020","Input Data Profile, Input Parameters, Number of CPU Cores, Workflow Structure (Spark DAG), Memory Size"
"Bellamy: Reusing Performance Models for Distributed Dataflow Jobs Across Contexts","2021","User Application Layer, Hardware Layer, Big Data Framework Layer, Data Layer","scheinert_bellamy_2021","Job parameters, ""Node Type (Master, Slave)"", Data Input Size, Target dataset characteristics, Number of CPU Cores, Memory Size, Workload Type"
"Enel: Context-Aware Dynamic Scaling of Distributed Dataflow Jobs using Graph Propagation","2021","Virtualisation Layer, Performance Layer, Data Layer, User Application Layer, Hardware Layer","scheinert_enel_2021","CPU Utilisation - Overall, Shuffle R/W, Data I/O, Ratio of memory spilled to disk to peak execution memory, Garbage collection time, Data Input Size, Input Parameters, Workload Type, hardware speciﬁcations / machine name"
"Efficient Performance Prediction for Apache Spark","2021","Big Data Framework Layer, Data Layer","cheng_efficient_2021","Data Input Size, Framework Parameter Configuration"
"Neural-based modeling for performance tuning of spark data analytics [arXiv]","2021","Performance Layer, User Application Layer","zaouk_neural-based_2021","Job Configuration, Processing Latency, Spark Time Metrics, CPU Utilisation - Overall, Network usage, I/O Usage, Input Parameters"
"Tuning configuration of apache spark on public clouds by combining multi-objective optimization and performance prediction model","2021","Big Data Framework Layer, Virtualisation Layer","cheng_tuning_2021","Framework Parameter Configuration, Number of Cloud Instances, Cloud Machine Type"
"A parallelization model for performance characterization of Spark Big Data jobs on Hadoop clusters","2021","Performance Layer, Big Data Framework Layer","ahmed_parallelization_2021","Task Execution Time, I/O Time, Framework Parameter Configuration, Available Resources, Available Network Bandwidth"
"Machine-Learning Based Memory Prediction Model for Data Parallel Workloads in Apache Spark","2021","Data Layer, User Application Layer","myung_machine-learning_2021","Workload Type, Size of elements"
"An Enhanced Parallelisation Model for Performance Prediction of Apache Spark on a Multinode Hadoop Cluster","2021","Hardware Layer, Performance Layer","ahmed_enhanced_2021","Task Memory Requirement, Number of CPU Cores, Number of Nodes"
"A performance modeling-based HADOOP configuration tuning strategy","2022","Big Data Framework Layer, Data Layer, User Application Layer","jie_performance_2022","Framework Parameter Configuration, Input Data, Workload Type"
"A Hybrid Machine Learning Approach for Performance Modeling of Cloud-Based Big Data Applications","2022","Data Layer, Hardware Layer, Performance Layer","ataie_hybrid_2022","Historical Execution Time, Number of Tasks, Job Data Size, Number of CPU Cores"
"MarVeLScaler: A Multi-View Learning-Based Auto-Scaling System for MapReduce","2022","Hardware Layer, Performance Layer, Big Data Framework Layer","li_marvelscaler_2022","Number of Nodes, Memory Size per Node, Type of the storage medium, CPU Processing Power, Task Execution Time, Task success rate, Number of Tasks"
"A Machine Learning Approach for Predicting Execution Statistics of Spark Application","2022","Big Data Framework Layer, Hardware Layer, Performance Layer","sewal_machine_2022","Number of Tasks, Historical Execution Time, Task I/O Read Cost, Task I/O Write Cost, Number of CPU Cores, Stage Execution Time, Task Execution Time"
"d-Simplexed: Adaptive Delaunay Triangulation or Performance Modeling and Prediction on Big Data Analytics","2022","Data Layer, Hardware Layer","chen_d-simplexed_2019","Data Input Size, Number of CPU Cores, Memory Size"
"Phronesis: Efficient Performance Modeling for High-dimensional Configuration Tuning","2022","Big Data Framework Layer","li_phronesis_2022","Framework Parameter Configuration"
"Cost-Aware Resource Recommendation for DAG-Based Big Data Workflows: An Apache Spark Case Study","2023","Big Data Framework Layer, Performance Layer","aseman-manzar_cost-aware_2022","Number of Slots, Number of Job Stages, Workflow Structure (Spark DAG), Historical Execution Time, Start and Cleanup Time"
"A Novel Multi-Task Performance Prediction Model for Spark","2023","Performance Layer, Big Data Framework Layer","shen_novel_2023","Spark History Logs, Framework Parameter Configuration, Historical Execution Time, CPU Utilisation - Overall, Memory Utilisation, Job Status, Ressource Utilisation Data, Task Execution Time"
"Fixed-Point Iteration Approach to Spark Scalable Performance Modeling and Evaluation","2023","Performance Layer","karimian-aliabadi_fixed-point_2023","Running Tasks, Completed Tasks, Available Resources"